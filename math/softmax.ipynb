{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在机器学习尤其是深度学习中，softmax是个非常常用而且比较重要的函数，尤其在多分类的场景中使用广泛。他把一些输入映射为0-1之间的实数，并且归一化保证和为1，因此多分类的概率之和也刚好为1。 \n",
    "\n",
    "首先我们简单来看看softmax是什么意思。顾名思义，softmax由两个单词组成，其中一个是$\\max$。对于$\\max$我们都很熟悉，比如有两个变量$a,b$。如果$a\\gt b$，则$\\max$为$a$，反之为$b$。用伪码简单描述一下就是 \n",
    "> if a > b return a; else b。 \n",
    "\n",
    "另外一个单词为soft, max方法存在的一个问题是什么呢？如果将$\\max$看成一个分类问题，就是非黑即白，最后的输出是一个确定的变量。更多的时候，我们希望输出的是取到某个分类的概率，或者说，我们希望分值大的那一项被经常取到，而分值较小的那一项也有一定的概率偶尔被取到，所以我们就应用到了soft的概念，即最后的输出是每个分类被取到的概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax expression\n",
    "假设存在一个向量$X = [a_1, a_2, \\ldots, a_m]$, 需要得到该向量元素的概率分布。如果$x$中的所有元素都为正数，那么可以直接用对应元素除以所有元素之后，得到的就是一个概率分布。\n",
    "\n",
    "但是$X$的元素取值范围为$- \\infty$到$+\\infty$,所以，应该先用一个非线性函数将$X$中的每一个值映射为一个正数，然后再归一化操作。\n",
    "\n",
    "所以，对$X$的每一个元素的进行softmax的公式为:\n",
    "$$\n",
    "S_i = \\frac{e^{a_i}}{\\sum_j e^{a_j}}\\tag{1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax求导\n",
    "通过公式(1)可以知道，$softmax(X)$是与数组$X$长度相同的向量。那么，对于$X$中的某个元素，得到的其梯度应该是一个长度等于$softmax(X)$的向量。\n",
    "\n",
    "具体地，输出元素$S_i$关于输入元素$a_j$的导数公式为:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial S_i}{\\partial a_j} = \n",
    "\\frac{\\frac{\\partial e^{a_i}}\n",
    "{\\partial a_j}\\cdot \\sum - \\frac{\\partial \\sum}{\\partial a_j}\\cdot e^{a_i}}{\\sum^2}\n",
    "\\tag{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个时候需要分成两个情况来讨论\n",
    "如果 $i = j$\n",
    "$$\n",
    "\\frac{\\partial S_i}{\\partial a_j} = S_j\\cdot(1- S_j) \\tag{3}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "否则，$i\\not= j$\n",
    "$$\n",
    "\\frac{\\partial S_i}{\\partial a_j} = -S_j\\cdot S_i\\tag{4}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么，写成向量的形式\n",
    "$$\n",
    "\\frac{\\partial S}{\\partial X} = \\text{diag}(S) - SS^T\\tag{5}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结合Cross Entropy\n",
    "在总多分类模型中，softmax的结果可以作为预测模型输入属于哪一个分类概率大小，也就是得到一个$Q(X)$，而真实世界中还有一个分布$P(X)$，模型应当是最大限度地拟合其在现实世界中的分布。所以通常选用交叉熵作为模型的损失函数。\n",
    "\n",
    "交叉熵的公式为:\n",
    "\n",
    "$$\n",
    "L = - \\displaystyle\\sum y_i\\log S_i\\tag{6}\n",
    "$$\n",
    "其中，$y_i$是X在真实世界中属于类别$i$的概率\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么，可以得到向量$X$的元素$a_i$关于交叉熵损失的导数为:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L}{\\partial a_i} \n",
    "&= \\displaystyle\\sum_j\\frac{\\partial L}{\\partial S_j}\\cdot\\frac{\\partial S_j}{\\partial a_i}\\\\\n",
    "&= \\frac{\\partial L}{\\partial S_i}\\cdot\\frac{\\partial S_i}{\\partial a_i}+ \\displaystyle\\sum_{j\\not=i}\\frac{\\partial L}{\\partial S_j}\\cdot\\frac{\\partial S_j}{\\partial a_i}\\\\\n",
    "&= -y_i\\cdot\\frac1{S_i}\\cdot S_i(1-S_i) + \n",
    "\\displaystyle\\sum_{j\\not=i}\\frac{-y_i}{S_j}\\cdot(-1)S_jS_i\\\\\n",
    "&= -y_i(1 - S_i) + \\sum_{j\\not=i}y_j\\cdot S_i\\\\\n",
    "&= S_i - y_i\n",
    "\\end{aligned}\n",
    "\\tag{7}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
