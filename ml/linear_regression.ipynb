{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性回归是利用数理统计中回归分析，来确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法，运用十分广泛。其表达形式为$y = xw + e$，$e$为误差服从均值为0的正态分布。\n",
    "\n",
    "回归分析中，只包括一个自变量和一个因变量，且二者的关系可用一条直线近似表示，这种回归分析称为一元线性回归分析。如果回归分析中包括两个或两个以上的自变量，且因变量和自变量之间是线性关系，则称为多元线性回归分析。\n",
    "\n",
    "线性回归的目的是为了将训练数据拟合到线性函数上。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于e服从均值为0的正太分布，我们假设其方差为$\\sigma^2$,那么有$P(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma}e^{- \\frac{(y - xw)^2}{2\\sigma^2}}$, 那么我们在拟合线性函数的时候，实际上是求使得给样本误差出现最大概率的模型参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么可以构造误差的MLE求参公式\n",
    "$$\n",
    "\\begin{aligned}\n",
    "W &= \\arg\\ \\underset{W}{\\max}\\displaystyle\\prod_i^n P(x_i)\\\\\n",
    "&= \\arg\\ \\underset{W}{\\max}\\displaystyle\\sum_i^n \\log P(x_i)\\\\\n",
    "&= \\arg\\ \\underset{W}{\\max}\\displaystyle\\sum_i^n -(y - xw)^2\\\\\n",
    "&= \\arg\\ \\underset{W}{\\min}\\displaystyle\\sum_i^n (y - xw)^2\\\\\n",
    "\\end{aligned}\\tag{1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "观察公式(1)可知，线性回归的模型参数使得模型误差平方和最小的。线性回归的目标函数通常写作均方误差和(MSE)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "令\n",
    "$$\n",
    "s(w) = (Y - Xw)^2\\tag{2}\n",
    "$$\n",
    "其中，$X$为训练样本，$Y$为样本真实函数值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么，对函数$s(w)$求微分，得到\n",
    "$$\n",
    "\\frac{\\partial s(x)}{\\partial w} = -2X^T(Y - Xw) = 0 \\tag{3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由公式(3)，有\n",
    "$$\n",
    "X^TXw = X^TY\\tag{4}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果$X^TX$非奇异(也就是该矩阵可逆)，$w$存在唯一解\n",
    "$$\n",
    "w = (X^TX)^{-1}X^TY\\tag{5}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述公式(5)实际上即是大名鼎鼎的最小二乘法。可以知道，线性回归通常能够求得其解析解。当然，也可以通过其优化目标函数得到最优解。\n",
    "\n",
    "给出一个Pytorch写的[示例](../code/ml/1_linear_regression.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
