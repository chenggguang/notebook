{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "笔记参考[矩阵求导术](https://zhuanlan.zhihu.com/p/24709748)\n",
    "\n",
    "矩阵求导可以说是机器学习中重要的一环，基本上所有参数优化方法都是梯度相关的。\n",
    "\n",
    "在介绍矩阵求导之前，需要了解什么是trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在线性代数中,一个$n\\times n$矩阵$A$的主对角线之和称为被称为矩阵$A$的迹(trace)，一般记为$tr(A)$\n",
    "$$\n",
    "tr(A) = \\displaystyle\\sum_{i = 1}^na_{ii}\n",
    "$$\n",
    "它具有如下性质\n",
    "\n",
    "- 对于标量: $a = tr(a)$\n",
    "- 矩阵转置: $tr(A^T) = tr(A)$\n",
    "- 线性运算: $tr(A\\pm B ) = tr(A)\\pm tr(B)$\n",
    "- 矩阵乘法交换: $tr(AB) = tr(BA)$，其中$A$与$B^T$的形状相同。\n",
    "- 矩阵逐元素乘法交换: $tr(A^T(B\\odot C)) = tr((A\\odot B)^TC)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 矩阵微分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "矩阵微分有如下性质\n",
    "\n",
    "- $d(X\\pm Y) = dX \\pm dY$\n",
    "- $d(XY) = (dX)Y + X(dY)$\n",
    "- $d(X^T) = (dX)^T$\n",
    "- $d(tr(X)) = tr(dX)$\n",
    "- $dX^{-1} = -X^{-1}dx X^{-1}$\n",
    "- $d|X| = tr(X^*dX)$\n",
    "- $d(X\\odot Y) = dX\\odot Y + X\\odot dY$\n",
    "- $d\\sigma(X) = \\sigma'(X)\\odot dX$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 矩阵求导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 标量对矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先考虑标量$f$对矩阵$X$的导数,其定义为$\\frac{\\partial f}{\\partial X} = \\left[\\frac{\\partial f}{\\partial X_{ij}} \\right]$,即$f$对$X$逐元素求导排成与$X$ shape一样的矩阵。这是不使用公式的解法。但是通常一个矩阵到一个标量会经过复杂的计算流程，直接求导计算过程十分繁琐，公式是必不可少的工具"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回顾一元微分中的导数与微分的关系$df = f'(x)dx$，多元微分中的梯度与微分的关系$df =\\displaystyle\\sum_{i = 1}^n \\frac{\\partial f}{\\partial x_i}dx_i =\\frac{\\partial f^T}{\\partial \\vec{x}}d\\vec{x}$，全微分$df$是梯度向量$\\frac{\\partial f}{\\partial \\vec{x}}$与微分向量$d\\vec{x}$的内积。那么，可以得到矩阵导数与微分的关系$df = \\displaystyle\\sum_{i = 1}^m\\sum_{j=1}^n \\frac{\\partial f}{\\partial X_{ij}}dX_{ij} = tr(\\frac{\\partial f^T}{\\partial X}dX)$。\n",
    "\n",
    "若标量函数$f$是矩阵$X$经加减乘法、逆、行列式、逐元素函数等运算构成，则使用相应的运算法则对$f$求微分，再使用迹技巧给$df$套上迹并将其它项交换至$dX$左侧，对照导数与微分的联系$df =tr(\\frac{\\partial f^T}{\\partial X}dX)$，就能得到导数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以$Y = AXB$为例，$Y$经过一个函数$f(\\cdot)$得到一个标量的结果。那么，求解$\\frac{\\partial f}{\\partial X}$。\n",
    "$$\n",
    "df = tr(\\frac{\\partial f^T}{\\partial Y}dY) \\tag{1}\\\\\n",
    "$$\n",
    "$$\n",
    "dY = dAXB + AdXB + AXdB \\tag{2} = AdxB\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结合(1)(2)有\n",
    "$$\n",
    "df = tr(\\frac{\\partial f^T}{\\partial Y}AdXB) = \n",
    "tr(B\\frac{\\partial f^T}{\\partial Y}AdX) = \n",
    "tr((A^T\\frac{\\partial f}{\\partial Y}B^T)^TdX)\\tag{3}\n",
    "$$\n",
    "所有$\\frac{df}{dX} = A^T\\frac{\\partial f}{\\partial Y}B^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 矩阵对矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于一个形状为$M\\times N$的矩阵$F$对形状为$P\\times Q$的矩阵$X$求导，一共有$M\\times N\\times P\\times Q$个偏导数$\\frac{\\partial F_{kl}}{\\partial X_{ij}}$。先来看一看向量对向量求导，得到一个雅各布矩阵\n",
    "$$\n",
    "\\frac{\\partial\\vec{f}}{\\partial \\vec{x}} = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial f_1}{\\partial x_1} &\n",
    "\\frac{\\partial f_2}{\\partial x_1} &\n",
    "\\ldots &\n",
    "\\frac{\\partial f_n}{\\partial x_1}\\\\\n",
    "\\frac{\\partial f_1}{\\partial x_2} &\n",
    "\\frac{\\partial f_2}{\\partial x_2} &\n",
    "\\ldots &\n",
    "\\frac{\\partial f_n}{\\partial x_2}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "\\frac{\\partial f_1}{\\partial x_m} &\n",
    "\\frac{\\partial f_2}{\\partial x_m} &\n",
    "\\ldots &\n",
    "\\frac{\\partial f_n}{\\partial x_m}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "仍然是满足$d\\vec{f} = \\frac{\\partial \\vec{f}^T}{\\partial \\vec{x}}d\\vec{x}$。\n",
    "\n",
    "再定义矩阵向量化(列优先)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "vec(X) = [X_{11}, \\ldots, X_{m1}, X_{12},\\ldots, \\ X_{m2}, \\ldots, X_{1n}, \\ldots,X_{mn}]^T\n",
    "$$\n",
    "并定义矩阵$F$对矩阵$X$的导数$\\frac{\\partial F}{\\partial X} = \\frac{\\partial vec(F)}{\\partial vec(X)}(PQ\\times MN)$\n",
    "\n",
    "矩阵向量化有如下性质\n",
    "\n",
    "- $vec(A + B) = vec(A)+vec(B)$\n",
    "- $vec(AXB) = (B^T\\otimes A)vec(X)$ $\\otimes$代表Kronecker积。\n",
    "- $vec(A^T) = K_{mn}vec(A)$，其中$K_{mn}$是交换矩阵，将按列优先的向量化变成按行优先。\n",
    "- $vec(A\\odot X) = diag(A)vec(X)$其中$diag(A)$是用A的元素按列优先排成的对角矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于Kronecker积，设矩阵$A \\in R^{M\\times N}, B\\in R^{S\\times T}$, 那么$A\\otimes B$是一个$MS \\times NT$的矩阵。\n",
    "$$\n",
    "[A\\otimes B] = \n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} & \\cdots & a_{1N}B \\\\\n",
    "a_{21} & a_{22} & \\cdots & a_{2N}B \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots  \\\\\n",
    "a_{M1} & a_{M2} & \\cdots & a_{MN}B \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Kronecker积的性质\n",
    "- $(A\\otimes B)^T = A^T \\otimes B^T$\n",
    "- $vec(ab^T) = b\\otimes a$\n",
    "- $(A\\otimes B)(C\\otimes D) = (AC)\\otimes(BD)$\n",
    "- $K_{mn} = K_{mn}^T,\\ K_{mn}K_{mn} = I$\n",
    "- $K_{pm}(A\\otimes B)K_{nq} = B\\otimes A$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "若矩阵函数$F$是矩阵$X$经加减乘法、逆、行列式、逐元素函数等运算构成，则使用相应的运算法则对$F$求微分，再做向量化并使用技巧将其它项交换至$vec(dX)$左侧，对照导数与微分的联系$d\\vec{f} = \\frac{\\partial \\vec{f}^T}{\\partial \\vec{x}}d\\vec{x}$就可以得到导数\n",
    "\n",
    "以求解$F = Ae^{XB}$为例\n",
    "$$\n",
    "dF = A\\big(e^{XB}\\odot (dXB)\\big)\\tag{4}\n",
    "$$\n",
    "向量化式(4)\n",
    "$$\n",
    "\\begin{aligned}\n",
    "vec(dF) &= vec\\Big(A\\big(e^{XB}\\odot (dXB)\\big)\\Big)\\\\\n",
    "&=(I\\otimes A)vec\\big(e^{XB}\\odot (dXB)\\big)\\\\\n",
    "&=(I\\otimes A)diag(e^{XB})vec(dXB)\\\\\n",
    "&=(I\\otimes A)diag(e^{XB})(B^T\\otimes I)vec(dX)\\\\\n",
    "&=\\big[(B\\otimes I)diag(e^{XB})(I\\otimes A^T)\\big]^Tvec(dX)\\\\\n",
    "\\end{aligned}\n",
    "\\tag{5}\n",
    "$$\n",
    "所以，得到$\\frac{dF}{dX} = (B\\otimes I)diag(e^{XB})(I\\otimes A^T)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 几个常用的求导公式\n",
    "- $\\frac{\\partial\\beta^TX}{\\partial X} = \\beta $\n",
    "- $\\frac{\\partial X^TX}{\\partial X} = 2X$\n",
    "- $\\frac{\\partial X^T A X}{\\partial X} = (A + A^T)X$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
